% Прогнозирование на основе кластеризации. Метод Уишарта.

Так как траектория системы прилегает к аттрактору и периодически проходит близко к самой себе в прошлом.
Разумно предположить, что если начало какого-то участка траектории близко к уже известному куску, называемому мотивом, то и продолжение этого участка совпадает с мотивом.

Как правило, в качестве мотивов выбираются центроиды кластеров в прострастве $z$-векторов, для чего необходимо уметь их кластеризовать.
Однако рассматриваемая постановка задачи омрачается тем, что неизвестно ни количество кластеров, ни даже его порядок -- десятки или миллионы, что не позволяет использовать многие из существующих алгоритмов кластеризации.

Одним из алгоритмов, которые можно использовать в данной задаче, является алгоритм Уишарта (Wishart), принимающий набор точек $V_n = \left\{ x_i \right\}_{i=1}^{n}$ и два параметра $k, h$.
Параметр $k$ представляет собой количество рассматриваемых ближайших соседей и довольно слабо влияет на конечный результат.
Параметр $h$ является пороговым значением, участвующим в определении того, считается ли некоторый набор точек <<значимым>>.
Более формально, класс $C \subseteq V_n$ называется значимым по высоте $h$, если
\begin{equation*}
    \max_{x_i, x_j \in C}
    \left| \frac{k}{n W(d_k(x_i))} - \frac{k}{n W(d_k(x_j))} \right| \geq h,
\end{equation*}
где $W(r)$ -- объем гипершара радиусом $r$, $d_k(x_i)$ -- расстояние от точки $x_i$ до ее $k$-го ближайшего соседа.
При малых значениях $h$ алгоритм возвращает множество кластеров, содержащих только одну точку, а при больших значениях $h$ практически все точки не относятся ни к одному из найденных кластеров, то есть считаются межкластерным шумом.

Результатом работы алгоритма является набор меток кластеров $w_i$, где метка $w_j = 0$ обозначает межкластерный шум. Алгоритм состоит из следующих шагов:
\begin{enumerate}
\item
    Сортируем все точки по возрастанию $d_k(x_i)$. После этой операции в начале списка находятся так называемые <<области сгущения>> -- точки, расположенные сравнительно близко к по крайней мере $k$ другим точкам;
\item
    Рассматриваем граф $G$, вершины которого будут соответствовать объектам $x_i$.
    Изначально этот граф не содержит ни одной вершины; вершины добавляются по одной в соответствии с порядком, в котором отсортированы точки.
    Если в какой-то момент времени граф содержит $n$ вершин, соответствующих $n$ первым точкам, то точка $x_{n+1}$ обрабатывается по следующим правилам:
    \begin{enumerate}
    \item
        В граф добавляется новая вершина $x_{n+1}$ (здесь и далее для простоты вершины графа обозначаются так же, как и исходные точки);
    \item
        Добавленная вершина соединяется со всеми вершинами $x_{j}, j \leq n$, для которых выполняется условие $d\left( x_{n+1}, x_j \right) \leq d_k\left( x_j \right)$ (не наоборот, так, что нельзя утверждать, что количество соседей не превосходит $k$);
    \item
        Если вершина $x_{n+1}$ оказывается изолированной, то относим эту точку к новому кластеру;
    \item
        Если вершина $x_{n+1}$ не изолирована и все ее соседи принадлежат одному и тому же кластеру, то относим эту точку к этому же кластеру;
    \item
        Если соседи $x_{n+1}$ принадлежат к различным кластерам, среди которых нет нулевого кластера (обозначающего межкластерный шум) и ровно один кластер является значимым, то присваиваем к этому значимому кластеру как саму точку $x_{n+1}$, так и всех ее соседей;
    \item
        В противном случае (то есть если соседи принадлежат к различным кластерам, причем или среди них есть нулевой, или количество значимых кластеров отлично от единицы),  % TODO: в лекции только кейс с более чем одним значимым, разве не может быть ноль?
        все кластера соседей вкупе с самой $x_{n+1}$ присоединяются к межкластерному шуму.
        % TODO: В лекциях также отмечено, что все значимые кластера считаются сфорированными и блокируются на добавление. Wakarimasen.
    \end{enumerate}
\end{enumerate}


При прогнозировании конец временного ряда сравнивается с началом каждого из обнаруженных мотивов.
Если расстояние между ними не превышает некоторого порога, то оставшаяся часть мотива дает предсказание на соответствующее число шагов вперед.
Так как количество полученных предсказаний может быть большим, то их необходимо агрегировать.
Как правило, рассматривается кластеризация множества предсказаний.
Если предсказания образуют один плотный кластер с небольшим количеством выбросов, то в качестве окончательного предсказания выбирается центроид этого кластера.
Если же обнаруживается несколько разнесенных кластеров сравнимой мощности, то точка считается непрогнозируемой.
