% Прогнозирование на основе кластеризации. Метод Уишарта.

Так как траектория системы прилегает к аттрактору и периодически проходит близко к самой себе в прошлом.
Разумно предположить, что если начало какого-то участка траектории близко к уже известному куску, называемому мотивом, то и продолжение этого участка совпадает с мотивом.

Как правило, в качестве мотивов выбираются центроиды кластеров в прострастве $z$-векторов, для чего необходимо уметь их кластеризовать.
Однако рассматриваемая постановка задачи омрачается тем, что неизвестно ни количество кластеров, ни даже его порядок -- десятки или миллионы, что не позволяет использовать многие из существующих алгоритмов кластеризации.

Одним из алгоритмов, которые можно использовать в данной задаче, является алгоритм Уишарта (Wishart), принимающий набор точек $V_n = \left\{ x_i \right\}_{i=1}^{n}$ и два параметра $k, h$.
Параметр $k$ представляет собой количество рассматриваемых ближайших соседей.
Параметр $h$ является пороговым значением, участвующим в определении того, считается ли некоторый набор точек <<значимым>>.
Более формально, класс $C \subseteq V_n$ называется значимым по высоте $h$, если
\begin{equation*}
    \max_{x_i, x_j \in C}
    \left| \frac{k}{n W(d_k(x_i))} - \frac{k}{n W(d_k(x_j))} \right| \geq h,
\end{equation*}
где $W(r)$ -- объем гипершара радиусом $r$, $d_k(x_i)$ -- расстояние от точки $x_i$ до ее $k$-го ближайшего соседа.

% TODO: cite "Алгоритм построения унимодальных кластеров методом $k$ ближайших соседей"
Результатом работы алгоритма является набор меток кластеров $w_i$, где метка $w_j = 0$ обозначает межкластерный шум. Алгоритм состоит из следующих шагов:
\begin{enumerate}
\item
    Сортируем все точки по возрастанию $d_k(x_i)$. После этой операции в начале списка находятся так называемые <<области сгущения>> -- точки, расположенные сравнительно близко к по крайней мере $k$ другим точкам;
\item
    Рассматриваем список кластеров, в начале работы алгоритма не содержащий ни одного кластера.
    Помимо информации о том, какие точки содержит данный кластер, будем хранить метку, указывающую, является ли кластер <<завершенным>>.
    В завершенные кластера новые точки не добавляются, хотя размер и номер завершенного кластера все еще может измениться за счет объединения его с другими кластерами.
    Кроме того, в соответствии с логикой работы алгоритма каждый завершенный кластер также является значимым по высоте $h$, что позволяет опустить часть проверок на значимость.
\item
    Рассматриваем граф $G$, вершины которого будут соответствовать объектам $x_i$.
    Изначально этот граф не содержит ни одной вершины; вершины добавляются по одной в соответствии с порядком, в котором отсортированы точки.
    Если в какой-то момент времени граф содержит $n$ вершин, соответствующих $n$ первым точкам, то точка $x_{n+1}$ обрабатывается по следующим правилам:
    \begin{enumerate}
    \item
        В граф добавляется новая вершина $x_{n+1}$ (здесь и далее для простоты вершины графа обозначаются так же, как и исходные точки);
    \item
        Добавленная вершина соединяется со всеми вершинами $x_{j}, j \leq n$, для которых выполняется условие $d\left( x_{n+1}, x_j \right) \leq d_k\left( x_j \right)$, то есть таких, что $x_{n+1}$ входит в список $k$ ближайших соседей точки $x_j$ (не наоборот);
    \item
        Если вершина $x_{n+1}$ оказывается изолированной, то относим эту точку к новому кластеру;
    \item
        Если вершина $x_{n+1}$ не изолирована и все ее соседи принадлежат одному и тому же кластеру, то относим эту точку либо к этому же кластеру, либо в шум, в зависимости от того, является ли кластер завершенным;
    \item
        Если соседи $x_{n+1}$ принадлежат к различным кластерам, среди которых нет нулевого кластера (обозначающего межкластерный шум) и не более одного кластера является значимым, то все кластера, включающие каких-либо соседей точки $x_{n+1}$ объединяются в один. Сама точка также относится к этому объединенному кластеру.
    \item
        В противном случае (то есть если соседи принадлежат к различным кластерам, причем или среди них есть нулевой, или количество значимых кластеров больше единицы), точка $x_{n+1}$ добавляется к шуму, а кластера, содержащие соседние точки, либо помечаются как завершенные (если они значимы по высоте $h$), либо также присоединяются к шуму.
    \end{enumerate}
\end{enumerate}


При прогнозировании конец временного ряда сравнивается с началом каждого из обнаруженных мотивов.
Если Евклидово расстояние между ними не превышает некоторого порога, то оставшаяся часть мотива, не участовавшая в определении расстояния по очевидным причинам, дает предсказание на соответствующее число шагов вперед.
Так как количество полученных предсказаний может быть большим, то их необходимо агрегировать.
Как правило, рассматривается кластеризация множества предсказаний.
Если предсказания образуют один плотный кластер с небольшим количеством выбросов, то в качестве окончательного предсказания выбирается центроид этого кластера.
Если же обнаруживается несколько разнесенных кластеров сравнимой мощности, равномерное распределение прогнозов или какая-либо другая ситуация, отличная от целевой, то точка считается непрогнозируемой.
