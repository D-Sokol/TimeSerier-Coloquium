% Прогнозирование в моделях регрессии. Безусловное прогнозирование. Условное прогнозирование. Прогнозирование при наличии авторегрессии ошибок.

Рассмотрим классическую регрессионную модель:
\begin{equation*}
    y = X \beta + \eps,
\end{equation*}
где $y$ -- вектор зависимых переменных, $X$ -- матрица независимых переменных, $\beta$ -- вектор параметров модели, $\eps$ -- вектор случайных ошибок, удовлетворяющих условиям Гаусса-Маркова.
Тогда под задачей прогнозирования понимается построение оценки для случайной величины $y_{n+1}$ для еще одного набора объясняющих переменных $x_{n+1}$ в предположении, что зависимая переменная удовлетворяет той же модели с теми же параметрами.

\paragraph{Безусловное прогнозирование}
В постановке безусловного прогнозирования считается, что вектор независимых переменных $x_{n+1}$ известен точно.
Тогда в качестве оценки параметров модели выберем МНК-оценки $\hat\beta, s^2$ а в качестве оценки $y_{n+1}$ следующую величину:
\begin{equation*}
    \hat{y} = x_{n+1}^T \hat\beta, \quad \hat\beta = \left( X^T X \right)^{-1} X^T y, \quad s^2 = \frac{e^T e}{n-k}.
\end{equation*}

Легко проверить, что оценка $\hat{y}$ является несмещенной и обладает наименьшей MSE среди всех линейных по $y$ несмещенных оценок:
\begin{equation*}
    \mean \left( \hat{y} - y_{n+1} \right)^2 =
    \sigma^2 \left( 1 + x_{n+1}^T \left( X^T X \right)^{-1} x_{n+1} \right).
\end{equation*}

Кроме того, если ошибки $\eps_1, \dots, \eps_{n+1}$ имеют в совокупности нормальное распределение, то нормированная ошибка
\begin{equation*}
    \frac{\hat{y} - y_{n+1}}{\sqrt{s^2 \left( 1 + x_{n+1}^T \left( X^T X \right)^{-1} x_{n+1} \right)}}
\end{equation*}
имеет распределение Стьюдента $t_{n-k}$, ($k$ -- количество независимых переменных) что позволяет построить доверительный интервал.


\paragraph{Условное прогнозирование}
В задаче условного прогнозирования считается, что вектор $x_{n+1}$ известен только с некоторой точностью, то есть фактически наблюдается вектор $z = x_{n+1} + u$, где $\mean u = 0, \var u = \sigma_u^2 I$, случайный вектор $u$ не зависит от всех ошибок $\eps_1, \dots, \eps_{n+1}$.
Тогда в качестве оценки для $y_{n+1}$ используется следующая величина:
\begin{equation*}
    \hat{y} = z^T \hat\beta.
\end{equation*}

Так как $\hat\beta$ зависит только от $\eps$, то $\hat\beta$ и $u$ независимы. Пользуясь этим фактом, легко показать, что эта оценка все еще является несмещенной:
\begin{equation*}
    \mean\left( \hat{y} - y_{n+1} \right) =
    \mean\left[\left( x_{n+1}^T + u^T \right) \hat\beta - x_{n+1}^T \beta \right] =
    x_{n+1}^T \left( \mean\hat\beta - \beta \right) + \left( \mean u \right)^T \left( \mean\hat\beta \right) =
    0,
\end{equation*}
и среднеквадратичная ошибка вычисляется как
\begin{equation*}
    \mean e^2 = \sigma^2 \left( 1 + x_{n+1}^T \left( X^T X \right)^{-1} x_{n+1} + \sigma_u^2 \tr \left( \left( X^T X \right)^{-1} \right) \right) + \sigma_u^2 \beta^T \beta.
\end{equation*}
Видно, что среднеквадратичная ошибка увеличивается и включает два новых слагаемых, пропорциональных дисперсии ошибки наблюдения $u$.
Однако аналитического выражения для построения доверительного интервала не существует.
% Такие дела, десу.


\paragraph{Авторегрессия ошибок}
Поскольку шум $\eps_t$ включает в себя все факторы, не учтенные моделью, то в реальных процессах ошибки не являются независимыми.
Поэтому рассмотрим более сложную модель, в которой ошибки образуют авторегрессионный процесс первого порядка:
\begin{equation*}
    \begin{cases}
        \eps_t = \rho \eps_{t-1} + \nu_t,\\
        y_t = x_t \cdot \beta + \eps_t,
    \end{cases}
\end{equation*}
где условиям Гаусса-Маркова удовлетворяют величины $\nu_t$, а не $\eps_t$; $|\rho| < 1$.
Более того, шум $\eps_t$ в различные моменты времени являются разными случайными величинами и могут иметь различные распределения.

Для начала предположим, что истинные параметры модели $\left( \rho, \beta \right)$ известны.
Тогда в качестве оценки $y_{n+1}$ возьмем
\begin{equation}
\label{11_1}
    \hat{y} =
    x_{n+1}^T \beta + \rho \left( y_n - x_n^T \beta \right).
\end{equation}

Легко проверить, что $e = \nu_{n+1}$, $\mean e = 0$, $\mean e^2 = \sigma_\nu^2 = \left( 1 - \rho^2 \right) \sigma_\eps^2$.
Отметим, что полученная среднеквадратичная ошибка меньше, чем в предположении некоррелированных ошибок и известного параметра $\beta$, когда $\mean e^2 = \sigma_\eps^2$.


В действительности параметры регрессии неизвестны.
Рассмотрим несколько стратегий построения оценки параметров модели:
\begin{enumerate}
\item
    Построение оценки $\left( \hat\beta, \hat\rho \right)$ по известным $x_i, y_i$, например, нелинейным методом наименьших квадратов.
\item
    Заметим, что при известном значении $\rho$ модель \eqref{11_1} можно свести к классической регрессионной модели с независимыми ошибками $\nu_t$, искусственно добавив новую независимую переменную $x_{t,k+1} = y_{t-1}$.
    Таким образом, при известном $\rho$ параметр $\beta$ может быть оценен при помощи описанных ранее методов.
    Поэтому можно перебрать значения $\rho$ в некотором промежутке, для каждого значения оценить $\beta$ и выбрать пару параметров, минимизирующих ошибку.
\item
    В дополнение к предыдущему пункту также заметим, что при известном значении $\beta$ значения $\eps_t$ также известны, поэтому параметр $\rho$ может быть оценен как параметр линейной регрессии.
    Следовательно, для оценки параметров можно применить следующий итерационный процесс:
    \begin{enumerate}
    \item
        Параметр $\rho$ оценивается любым способом, например, методом случайного угадывания;
    \item
        Считая полученную на предыдущем шаге оценку $\hat\rho$  истинным значением, вычисляем оценку параметра $\beta$;
    \item
        Считая полученную на предыдущем шаге оценку $\hat\beta$ истинным значением, вычисляем оценку параметра $\rho$ ;
    \item
        Шаги 2-3 повторяются до достижения сходимости.
    \end{enumerate}
\end{enumerate}

