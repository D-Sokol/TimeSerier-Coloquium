% Плоскость энтропия-сложность.

Рассмотрим временной ряд $x_t$ и соответствующий ему ряд из $z$-векторов размерности $d$:
\begin{equation*}
    \vec z_i = \begin{pmatrix}
        x_i & x_{i+1} & \dots & x_{i+d-1}
    \end{pmatrix}^T.
\end{equation*}

Каждому такому вектору сопоставляется перестановка $\sigma \in S_d$, сортирующая этот вектор, то есть такая, что
\begin{equation*}
    z_{\sigma^{-1}_0} \leq z_{\sigma^{-1}_1} \leq \dots \leq z_{\sigma^{-1}_{d-1}}.
\end{equation*}

Заметим, что в случае, когда некоторые элементы вектора одинаковы, такая перестановка не единственна, но в случае непрерывного фазового пространства этой возможностью можно пренебречь.
Также можно рассматривать не саму перестановку $\sigma$, а обратную к ней $\sigma^{-1}$. Такая замена приведет только к изменению порядка чисел, описывающих эмпирическое распределение, но не повлияет на конечный результат.
В описанном случае перестановка $\sigma$ может быть получена программно при помощи функции \texttt{argsort}, определенной во многих библиотеках.

Поскольку элементы временного ряда $x_i$ можно рассматривать как наблюдения некоторого временного процесса, $\sigma(z)$ является дискретной случайной величиной, обладающей каким-то распределением $P$.

Если рассматриваемый процесс является шумом, то $P$ -- это равномерное распределение $U$ на множестве $S_d$.
А хаотической динамической системе отвечает некое распределение $P \neq U$.
\footnote{Регулярной динамической системе также отвечает распределение, не являющееся равномерным, но к таким системам этот метод исследования обычно не применяется.}

Случайное распределение можно описать в терминах энтропии и неравновесности. Дадим соответствующие определения:
\begin{definition}[Информация по Шеннону]
    Рассмотрим произвольное распределение $P$ и событие $A$. Тогда \textbf{информацией Шеннона}, соответствующей этому событию, называется значение $I_A = f\left( \frac{1}{P(A)} \right)$, где $f$ -- некоторая функция, удовлетворяющая следующим свойствам:
    \begin{itemize}
    \item
        Функция $f$ возрастает;
    \item
        Если события $A, B$ независимы, то $I_{AB} = I_{A} + I_{B}$.
    \end{itemize}
    
    Можно показать, что этим условиям удовлетворяет только логарифмическая функция с произвольным основанием. В целях нормировки общепринято использование двоичного логарифма; единицей измерения информации в таком случае является бит. Таким образом, получаем:
    \begin{equation*}
        I_A = \log_2 \frac{1}{P(A)} = -\log_2 P(A).
    \end{equation*}
\end{definition}

\begin{definition}[Энтропия по Шеннону]
    \textbf{Энтропией по Шеннону} называется среднее количество информации, получаемое в результате одного наблюдения:
    \begin{equation*}
        H = \mean_{P} I_{\omega} = \sum_{i} I_{\omega_i} p_i = -\sum_i p_i \log p_i.
    \end{equation*}
    
    Известно, что энтропия принимает значения от $0$ до $\log N$, где $N$ -- количество элементарных событий.
    Минимальное значение принимается в случае вырожденного распределения ($p_i = \delta_{ij}$), максимальное -- в случае равномерного распределения. Также часто рассматривается \textbf{нормализованная энтропия}:
    \begin{equation*}
        \bar{H}(P) = \frac{H(P)}{H_{\text{max}}} = \frac{H(P)}{H(U)} = \frac{H(P)}{\log N} \in \left[ 0, 1 \right].
    \end{equation*}
\end{definition}

\begin{definition}[Сложность]
    Сложностью называется следующая величина:
    \begin{equation*}
        S = Q\left[ P, U \right] \cdot \bar{H}\left( P \right),
    \end{equation*}
    где неравновесность $Q$ -- некоторая метрика расстояния между распределениями, нормированная в диапазон от 0 до 1.

    Будем использовать в качестве метрики расстояния \textbf{дивергенцию Йенсена-Шеннона}:
    \begin{equation*}
        Q\left[ P_1, P_2 \right] = Q_0 \cdot \left( H\left( \frac{P_1 + P_2}{2} \right) - \frac{H(P_1) + H(P_2)}{2} \right),
    \end{equation*}
    $Q_0 = -2\left( \left( \frac{N+1}{N} \right) \ln (N+1) - 2 \ln (2N) + \ln N \right)^{-1}$ -- нормировочный коэффициент.
    % TODO: cite "Generalized statistical complexity measures Geometrical and analytical properties"
\end{definition}

Возможные положения точки $(\bar{H}, S)$ на плоскости <<энтропия-сложность>> заключены между двумя кривыми, $0 \leq H \leq 1, 0 \leq S \leq \frac{1}{2}$. При этом верхней части области соответствуют хаотические временные ряды, нижним областям -- шум.
