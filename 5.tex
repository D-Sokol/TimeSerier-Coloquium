% Плоскость энтропия-сложность.

Рассмотрим временной ряд $x_t$ и соответствующий ему ряд из $z$-векторов размерности $d$:
\begin{equation*}
    \vec z_i = \begin{pmatrix}
        x_i & x_{i+1} & \dots & x_{i+d-1}
    \end{pmatrix}^T.
\end{equation*}

Каждому такому вектору сопоставляется набор логических значений $f(z) \in \Omega = \left\{ 0, 1 \right\}^{d-1}$ по следующему правилу:
\begin{equation*}
    f:
    z = \begin{pmatrix}
        x_{1} \\ x_{2} \\ \dots \\ x_{d}
    \end{pmatrix}
    \mapsto
    \begin{pmatrix}
        \left[ x_1 \leq x_2 \right] \\
        \left[ x_2 \leq x_3 \right] \\
        \dots \\
        \left[ x_{d-1} \leq x_{d} \right] \\
    \end{pmatrix}
    \in \Omega.
\end{equation*}
% Чей-то могучий интеллект обозвал эти значения "перестановками". Мы решительно выступаем против подобной терминологии.

Поскольку элементы временного ряда $x_i$ можно рассматривать как наблюдения некоторого временного процесса, $f(z)$ является дискретной случайной величиной, обладающей каким-то распределением $P$.

Если рассматриваемый процесс является шумом, то $P$ -- это равномерное распределение $U$ на множестве $\Omega$.
А хаотической динамической системе отвечает некое распределение $P \neq U$.
\footnote{Регулярной динамической системе также отвечает распределение, не являющееся равномерным, но к таким системам этот метод исследования обычно не применяется.}

Случайное распределение можно описать в терминах энтропии и неравновесности. Дадим соответствующие определения:
\begin{definition}[Информация по Шеннону]
    Рассмотрим произвольное распределение $P$ и событие $A$. Тогда \textbf{информацией Шеннона}, соответствующей этому событию, называется значение $I_A = f\left( \frac{1}{P(A)} \right)$, где $f$ -- некоторая функция, удовлетворяющая следующим свойствам:
    \begin{itemize}
    \item
        Функция $f$ возрастает;
    \item
        Если события $A, B$ независимы, то $I_{AB} = I_{A} + I_{B}$.
    \end{itemize}
    
    Можно показать, что этим условиям удовлетворяет только логарифмическая функция с произвольным основанием. В целях нормировки общепринято использование двоичного логарифма; единицей измерения информации в таком случае является бит. Таким образом, получаем:
    \begin{equation*}
        I_A = \log_2 \frac{1}{P(A)} = -\log_2 P(A).
    \end{equation*}
\end{definition}

\begin{definition}[Энтропия по Шеннону]
    \textbf{Энтропией по Шеннону} называется среднее количество информации, получаемое в результате одного наблюдения:
    \begin{equation*}
        H = \mean_{P} I_{\omega} = \sum_{i} I_{\omega_i} p_i = -\sum_i p_i \log p_i.
    \end{equation*}
    
    Известно, что энтропия минимальна и равна 0 в случае вырожденного распределения ($p_i = \delta_{ij}$) и максимальна в случае равномерного распределения. Также часто рассматривается \textbf{нормализованная энтропия}:
    \begin{equation*}
        \bar{H}(P) = \frac{H(P)}{H_{\text{max}}} = \frac{H(P)}{H(U)} \in \left[ 0, 1 \right].
    \end{equation*}
\end{definition}

\begin{definition}[Неравновесность (сложность)]
    Неравновесностью называется следующая величина:
    \begin{equation*}
        S = Q\left[ P, U \right] \cdot \bar{H}\left( P \right),
    \end{equation*}
    где расстояние между распределениями $Q$ задается следующей формулой:
    \begin{equation*}
        Q\left[ P_1, P_2 \right] = Q_0 \cdot \left( 2H\left( \frac{P_1 + P_2}{2} \right) - H(P_1) - H(P_2) \right),
    \end{equation*}
    $Q_0$ -- нормировочный коэффициент.
\end{definition}

При правильном выборе нормировочных коэффициентов возможные положения точки $(\bar{H}, S)$ на плоскости <<энтропия-сложность>> заключены между двумя параболами, $0 \leq H \leq 1, 0 \leq S \leq \frac{1}{2}$. При этом верхней части области соответствуют хаотические временные ряды, нижним областям -- шум.
